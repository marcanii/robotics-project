{"cells":[{"cell_type":"markdown","metadata":{"id":"4Kzp2jHl3qY7"},"source":["# Mecanismos de Atenci√≥n"]},{"cell_type":"markdown","metadata":{"id":"V6DHbAbUOo9K"},"source":["En el [post](https://sensioai.com/blog/040_encoder_decoder) anterior aprendimos a implementar una arquitectura de red neuronal conocida como `seq2seq`, que utiliza dos redes neuronales (el `encoder` y el `decoder`) para poder trabajar con secuencias de longitud arbitraria tanto a sus entradas como en las salidas. Este modelo nos permite llevar a cabo tareas tales como la traducci√≥n de texto entre dos idiomas, resumir un texto, responder preguntas, etc.\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","Si bien este modelo nos dio buenos resultados, podemos mejorarlo. Si prestamos atenci√≥n a la arquitectura que desarrollamos, el `decoder` (encargado de generar la secuencia de salida) es inicializado con el √∫ltimo estado oculto del `encoder`, el cual tiene la responsabilidad de codificar el significado de toda la frase original. Esto puede ser complicado, sobre todo al trabajar con secuencias muy largas, y para solventar este problema podemos utilizar un mecanismo de `atenci√≥n` que no solo reciba el √∫ltimo estado oculto si no tambi√©n tenga acceso a todas las salidas del `encoder` de manera que el `decoder` sea capaz de \"focalizar su atenci√≥n\" en aquellas partes m√°s importantes. Por ejemplo, para traducir la primera palabra es l√≥gico pensar que lo m√°s importante ser√° la primera palabra y sus adyacentes en la frase original, pero usar el √∫ltimo estado oculto del `encoder` puede no ser suficiente para mantener estas relaciones a largo plazo. Permitir al `decoder` acceder a esta informaci√≥n puede resultar en mejores prestaciones."]},{"cell_type":"markdown","metadata":{"id":"zDH7Z9YXOo9K"},"source":["> üí° En la pr√°ctica, los mecanismos de atenci√≥n dan muy buenos resultados en tareas que envuelvan datos secuenciales (como aplicaciones de lenguaje). De hecho, los mejores modelos a d√≠a de hoy para tareas de `NLP` no est√°n basados en redes recurrentes sino en arquitecturas que √∫nicamente implementan mecanismos de atenci√≥n en varias capas. Estas redes neuronales son conocidas como `Transformers`."]},{"cell_type":"markdown","metadata":{"id":"cZTygGNgOo9L"},"source":["## El *dataset*"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-03T08:31:48.390280Z","start_time":"2020-09-03T08:31:48.382280Z"},"id":"i6RafGqyOo9L"},"source":["Vamos a resolver exactamente el mismo caso que en el post anterior, as√≠ que todo lo que hace referencia al procesado de datos lo dejaremos igual."]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:52.733066Z","start_time":"2020-09-04T12:31:52.725066Z"},"id":"fa84Y6t8Oo9L"},"outputs":[],"source":["import unicodedata\n","import re\n","\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","def read_file(file, reverse=False):\n","    # Read the file and split into lines\n","    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n","\n","    return pairs"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:56.056055Z","start_time":"2020-09-04T12:31:52.735065Z"},"id":"xLRDgMH_Oo9M"},"outputs":[],"source":["pairs = read_file('spa.txt')"]},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:56.071561Z","start_time":"2020-09-04T12:31:56.058156Z"},"id":"SIDvWOueOo9M","outputId":"c05e48c5-46c2-45e8-8b78-513ab9073c50"},"outputs":[{"data":{"text/plain":["['you know that .', 'ustedes los saben .']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import random\n","\n","random.choice(pairs)"]},{"cell_type":"code","execution_count":6,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:56.087569Z","start_time":"2020-09-04T12:31:56.074570Z"},"id":"HvTf8H0BOo9N"},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","PAD_token = 2\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n","        self.n_words = 3  # Count SOS, EOS and PAD\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","            \n","    def indexesFromSentence(self, sentence):\n","        return [self.word2index[word] for word in sentence.split(' ')]\n","    \n","    def sentenceFromIndex(self, index):\n","        return [self.index2word[ix] for ix in index]"]},{"cell_type":"markdown","metadata":{"id":"j50IarGMOo9N"},"source":["Para poder aplicar la capa de `attention` necesitamos que nuestras frases tengan una longitud m√°xima definida."]},{"cell_type":"code","execution_count":9,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:56.103564Z","start_time":"2020-09-04T12:31:56.088570Z"},"id":"sEfTtGdCOo9N"},"outputs":[],"source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","def filterPairs(pairs, filters, lang=0):\n","    return [p for p in pairs if p[lang].startswith(filters)]\n","\n","def trimPairs(pairs):\n","    return [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]"]},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:59.717657Z","start_time":"2020-09-04T12:31:56.104565Z"},"code_folding":[0],"id":"ZJNB1H19Oo9O","outputId":"b79e490d-2c64-4ad1-9cfb-d4cd08094854"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tenemos 134142 pares de frases\n","Tenemos 102757 pares de frases con longitud menor de 10\n","Longitud vocabularios:\n","spa 11253\n","eng 21418\n"]},{"data":{"text/plain":["['it is probable that she will come . EOS', 'ella probablemente vendra . EOS']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def prepareData(file, filters=None, reverse=False):\n","    \n","    pairs = read_file(file, reverse)\n","    print(f\"Tenemos {len(pairs)} pares de frases\")\n","    \n","    if filters is not None:\n","        pairs = filterPairs(pairs, filters, int(reverse))\n","        print(f\"Filtramos a {len(pairs)} pares de frases\")\n","        \n","    pairs = trimPairs(pairs)\n","    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {MAX_LENGTH}\")\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang('eng')\n","        output_lang = Lang('spa')\n","    else:\n","        input_lang = Lang('spa')\n","        output_lang = Lang('eng')\n","    \n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","        \n","        # add <eos> token\n","        pair[0] += \" EOS\"\n","        pair[1] += \" EOS\"\n","                           \n","    print(\"Longitud vocabularios:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","                           \n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('spa.txt')\n","\n","# descomentar para usar el dataset filtrado\n","#input_lang, output_lang, pairs = prepareData('spa.txt', filters=eng_prefixes)\n","                           \n","random.choice(pairs)"]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:59.730653Z","start_time":"2020-09-04T12:31:59.719659Z"},"id":"oTLfkd2tOo9O","outputId":"8c4171fd-a3bd-4a31-9c7a-cec8a9367dc2","scrolled":true},"outputs":[{"data":{"text/plain":["[72, 5391, 143, 4]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["output_lang.indexesFromSentence('tengo mucha sed .')"]},{"cell_type":"code","execution_count":12,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:31:59.754653Z","start_time":"2020-09-04T12:31:59.731654Z"},"id":"_K0lprkiOo9O","outputId":"1854b07a-a653-48d8-8057-f77292810f47"},"outputs":[{"data":{"text/plain":["['tengo', 'mucha', 'sed', '.']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["output_lang.sentenceFromIndex([72, 5391, 143, 4])"]},{"cell_type":"markdown","metadata":{"id":"UCx48yWVOo9P"},"source":["En el `Dataset` nos aseguraremos de a√±adir el *padding* necesario para que todas las frases tengan la misma longitud, lo cual no hace necesario utilizar la funci√≥n `collate` que implementamos en el post anterior."]},{"cell_type":"code","execution_count":13,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:00.233655Z","start_time":"2020-09-04T12:31:59.756655Z"},"code_folding":[],"id":"nGHATYuqOo9P","outputId":"0ce4f406-c6b1-4c5d-fc56-a961d8584f09"},"outputs":[{"data":{"text/plain":["(82205, 20552)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, input_lang, output_lang, pairs, max_length):\n","        self.input_lang = input_lang\n","        self.output_lang = output_lang\n","        self.pairs = pairs\n","        self.max_length = max_length\n","    \n","    def __len__(self):\n","        return len(self.pairs)\n","        \n","    def __getitem__(self, ix):        \n","        inputs = torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long)\n","        outputs = torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n","        # metemos padding a todas las frases hast a la longitud m√°xima\n","        return torch.nn.functional.pad(inputs, (0, self.max_length - len(inputs)), 'constant', self.input_lang.word2index['PAD']), \\\n","            torch.nn.functional.pad(outputs, (0, self.max_length - len(outputs)), 'constant', self.output_lang.word2index['PAD'])\n","\n","# separamos datos en train-test\n","train_size = len(pairs) * 80 // 100 \n","train = pairs[:train_size]\n","test = pairs[train_size:]\n","\n","dataset = {\n","    'train': Dataset(input_lang, output_lang, train, max_length=MAX_LENGTH),\n","    'test': Dataset(input_lang, output_lang, test, max_length=MAX_LENGTH)\n","}\n","\n","len(dataset['train']), len(dataset['test'])"]},{"cell_type":"code","execution_count":14,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.359235Z","start_time":"2020-09-04T12:32:00.234660Z"},"id":"nmJQ-cwdOo9P","outputId":"a511c955-e236-4372-f871-18783abb8951"},"outputs":[{"data":{"text/plain":["(tensor([3, 4, 1, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'),\n"," tensor([5, 4, 1, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'))"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["input_sentence, output_sentence = dataset['train'][1]\n","\n","input_sentence, output_sentence"]},{"cell_type":"code","execution_count":15,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.374232Z","start_time":"2020-09-04T12:32:01.360239Z"},"id":"ZV9Jej5MOo9P","outputId":"45c29aa4-d41c-4dca-9e3e-ae38d8a9e8ce"},"outputs":[{"data":{"text/plain":["(['go', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'],\n"," ['vete', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"]},{"cell_type":"code","execution_count":16,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.405231Z","start_time":"2020-09-04T12:32:01.375236Z"},"id":"nOuvr6KGOo9Q","outputId":"b3526bf1-d111-4808-a43b-87a07b732ac1"},"outputs":[{"data":{"text/plain":["(torch.Size([64, 10]), torch.Size([64, 10]))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["dataloader = {\n","    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True),\n","    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False),\n","}\n","\n","inputs, outputs = next(iter(dataloader['train']))\n","inputs.shape, outputs.shape"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-04T08:13:53.670033Z","start_time":"2020-09-04T08:13:53.652976Z"},"id":"qYDVBT25Oo9Q"},"source":["## El modelo"]},{"cell_type":"markdown","metadata":{"id":"udVDfpZhOo9Q"},"source":["En lo que se refiere al `encoder`, seguimos usando exactamente la misma arquitectura. La √∫nica diferencia es que, adem√°s del √∫ltimo estado oculto, necesitaremos todas sus salidas para que el `decoder` pueda usarlas."]},{"cell_type":"code","execution_count":17,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.421231Z","start_time":"2020-09-04T12:32:01.406231Z"},"id":"3-6lN-s0Oo9Q"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n","        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n","\n","    def forward(self, input_sentences):\n","        embedded = self.embedding(input_sentences)\n","        outputs, hidden = self.gru(embedded)\n","        return outputs, hidden"]},{"cell_type":"code","execution_count":19,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.452231Z","start_time":"2020-09-04T12:32:01.422235Z"},"id":"t6I1j6pLOo9Q","outputId":"b9cc3915-3d2c-4e6d-b90a-72787b5869c5"},"outputs":[{"data":{"text/plain":["torch.Size([64, 10, 100])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["encoder = Encoder(input_size=input_lang.n_words)\n","encoder_outputs, encoder_hidden = encoder(torch.randint(0, input_lang.n_words, (64, 10)))\n","\n","# [batch size, seq len, hidden size]\n","encoder_outputs.shape"]},{"cell_type":"code","execution_count":20,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.468231Z","start_time":"2020-09-04T12:32:01.453237Z"},"id":"P7I86qgfOo9R","outputId":"c1a6e084-2266-41e3-be7a-87ac26ea59f3"},"outputs":[{"data":{"text/plain":["torch.Size([2, 64, 100])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# [num layers, batch size, hidden size]\n","encoder_hidden.shape"]},{"cell_type":"markdown","metadata":{"id":"oliHI0C_Oo9R"},"source":["### El *decoder* con *attention*"]},{"cell_type":"markdown","metadata":{"id":"qwSpWTf8Oo9R"},"source":["Vamos a ver un ejemplo de implementaci√≥n de una capa de atenci√≥n para nuestro `decoder`. En primer lugar tendremos una capa lineal que recibir√° como entradas los `embeddings` y el estado oculto anterior (concatenados). Esta capa lineal nos dar√° a la salida tantos valores como elementos tengamos en nuestras secuencias de entrada (recuerda que las hemos forzado a tener una longitud determinada). Despu√©s, aplicaremos una funci√≥n `softmax` sobre estos valores obteniendo as√≠ una distribuci√≥n de probabilidad que, seguidamente, multiplicaremos por los *outputs* del encoder (que tambi√©n tienen la misma longitud). En esta funci√≥n de probabilidad, cada elemento tiene un valor entre 0 y 1. As√≠ pues, esta operaci√≥n dar√° m√°s importancia a aquellos *outputs* del `encoder` m√°s importantes mientras que al resto les asignar√° unos valores cercanos a 0. A continuaci√≥n, concatenaremos estos valores con los `embeddings`, de nuevo, y se lo daremos a una nueva capa lineal que combinar√° estos `embeddings` con los *outputs* del `encoder` re-escalados para obtener as√≠ los *inputs* finales de la capa recurrente. \n","\n","En resumen, usaremos las entradas y estado oculto del `decoder` para encontrar unos pesos que re-escalar√°n las salidas del `encoder`, los cuales combinaremos de nuevo con las entradas del `decoder` para obtener las representaciones finales de nuestras frases que alimentan la capa recurrente.\n","\n","![](https://i.imgur.com/1152PYf.png)"]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T13:43:51.447608Z","start_time":"2020-09-04T13:43:51.433585Z"},"id":"B6kAimFKOo9R"},"outputs":[],"source":["class AttnDecoder(torch.nn.Module):\n","    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2, max_length=MAX_LENGTH):\n","        super().__init__()\n","\n","        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n","        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n","        self.out = torch.nn.Linear(hidden_size, input_size)\n","        \n","        # attention\n","        self.attn = torch.nn.Linear(hidden_size + embedding_size, max_length)\n","        self.attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n","\n","\n","    def forward(self, input_words, hidden, encoder_outputs):\n","        # sacamos los embeddings\n","        embedded = self.embedding(input_words)\n","        # calculamos los pesos de la capa de atenci√≥n\n","        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1))) \n","        # re-escalamos los outputs del encoder con estos pesos\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n","        output = torch.cat((embedded.squeeze(1), attn_applied.squeeze(1)), 1)\n","        # aplicamos la capa de atenci√≥n\n","        output = self.attn_combine(output)\n","        output = torch.nn.functional.relu(output)\n","        # a partir de aqu√≠, como siempre. La diferencia es que la entrada a la RNN\n","        # no es directmanete el embedding sino una combinaci√≥n del embedding\n","        # y las salidas del encoder re-escaladas\n","        output, hidden = self.gru(output.unsqueeze(1), hidden)\n","        output = self.out(output.squeeze(1))        \n","        return output, hidden, attn_weights"]},{"cell_type":"code","execution_count":23,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.539231Z","start_time":"2020-09-04T12:32:01.484236Z"},"id":"DZ2mQFX7Oo9R","outputId":"bed0bdb1-e7f7-4ec8-de52-ff6bcb92647a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\israe\\AppData\\Local\\Temp\\ipykernel_19072\\1161045504.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n"]},{"data":{"text/plain":["torch.Size([64, 21418])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["decoder = AttnDecoder(input_size=output_lang.n_words)\n","decoder_output, decoder_hidden, attn_weights = decoder(torch.randint(0, output_lang.n_words, (64, 1)), encoder_hidden, encoder_outputs)\n","\n","# [batch size, vocab size]\n","decoder_output.shape"]},{"cell_type":"code","execution_count":24,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.547232Z","start_time":"2020-09-04T12:32:01.541233Z"},"id":"3CfnHo7cOo9S","outputId":"441c7af6-5faa-4f06-fdf8-1d670de50a61"},"outputs":[{"data":{"text/plain":["torch.Size([2, 64, 100])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# [num layers, batch size, hidden size]\n","decoder_hidden.shape"]},{"cell_type":"code","execution_count":25,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.572231Z","start_time":"2020-09-04T12:32:01.548231Z"},"id":"PqNrJp4QOo9S","outputId":"8157ccb0-8d71-413e-8ef1-a648d14fad8a"},"outputs":[{"data":{"text/plain":["torch.Size([2, 64, 100])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# [num layers, batch size, hidden size]\n","decoder_hidden.shape"]},{"cell_type":"code","execution_count":26,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.578233Z","start_time":"2020-09-04T12:32:01.573232Z"},"id":"eRW0a4o1Oo9S","outputId":"8e28d91c-25b7-4597-e85d-34cdac5beae7"},"outputs":[{"data":{"text/plain":["torch.Size([64, 10])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# [batch size, max_length]\n","attn_weights.shape"]},{"cell_type":"markdown","metadata":{"id":"Uilv4pINOo9S"},"source":["## Entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"amqXOG92Oo9S"},"source":["Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generar√° la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."]},{"cell_type":"code","execution_count":27,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:32:01.593231Z","start_time":"2020-09-04T12:32:01.579232Z"},"code_folding":[3],"id":"gY7YTd2NOo9S"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","def fit(encoder, decoder, dataloader, epochs=10):\n","    encoder.to(device)\n","    decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(1, epochs+1):\n","        encoder.train()\n","        decoder.train()\n","        train_loss = []\n","        bar = tqdm(dataloader['train'])\n","        for batch in bar:\n","            input_sentences, output_sentences = batch\n","            bs = input_sentences.shape[0]                    \n","            loss = 0\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            # obtenemos el √∫ltimo estado oculto del encoder\n","            encoder_outputs, hidden = encoder(input_sentences)\n","            # calculamos las salidas del decoder de manera recurrente\n","            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n","            for i in range(output_sentences.shape[1]):\n","                output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n","                loss += criterion(output, output_sentences[:, i].view(bs))     \n","                # el siguiente input ser√° la palabra predicha\n","                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n","            # optimizaci√≥n\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            train_loss.append(loss.item())\n","            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n","            \n","        val_loss = []\n","        encoder.eval()\n","        decoder.eval()\n","        with torch.no_grad():\n","            bar = tqdm(dataloader['test'])\n","            for batch in bar:\n","                input_sentences, output_sentences = batch\n","                bs = input_sentences.shape[0]  \n","                loss = 0\n","                # obtenemos el √∫ltimo estado oculto del encoder\n","                encoder_outputs, hidden = encoder(input_sentences)\n","                # calculamos las salidas del decoder de manera recurrente\n","                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n","                for i in range(output_sentences.shape[1]):\n","                    output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n","                    loss += criterion(output, output_sentences[:, i].view(bs))     \n","                    # el siguiente input ser√° la palabra predicha\n","                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n","                val_loss.append(loss.item())\n","                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"]},{"cell_type":"code","execution_count":28,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T12:58:02.367102Z","start_time":"2020-09-04T12:32:01.595236Z"},"id":"AfBqMLJaOo9T","outputId":"a3bbeaea-f576-458b-9c92-ab5f0dbe11f9"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\israe\\anaconda3\\envs\\cuda_user\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","  0%|          | 0/1285 [00:00<?, ?it/s]C:\\Users\\israe\\AppData\\Local\\Temp\\ipykernel_19072\\1161045504.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n","Epoch 1/30 loss 34.72180: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:36<00:00, 35.55it/s]\n","Epoch 1/30 val_loss 46.53839: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.83it/s]\n","Epoch 2/30 loss 27.81580: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.75it/s]\n","Epoch 2/30 val_loss 43.18733: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.02it/s]\n","Epoch 3/30 loss 24.54801: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.88it/s]\n","Epoch 3/30 val_loss 41.25538: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.97it/s]\n","Epoch 4/30 loss 22.19214: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.76it/s]\n","Epoch 4/30 val_loss 40.00292: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.83it/s]\n","Epoch 5/30 loss 20.35065: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.79it/s]\n","Epoch 5/30 val_loss 39.26364: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.47it/s]\n","Epoch 6/30 loss 18.85470: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.96it/s]\n","Epoch 6/30 val_loss 38.86445: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.39it/s]\n","Epoch 7/30 loss 17.56935: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:38<00:00, 33.41it/s]\n","Epoch 7/30 val_loss 38.78302: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 21.67it/s]\n","Epoch 8/30 loss 16.47182: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:40<00:00, 31.56it/s]\n","Epoch 8/30 val_loss 38.29305: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 26.48it/s]\n","Epoch 9/30 loss 15.52035: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:34<00:00, 36.73it/s]\n","Epoch 9/30 val_loss 38.31229: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.96it/s]\n","Epoch 10/30 loss 14.67717: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 36.24it/s]\n","Epoch 10/30 val_loss 38.63814: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.51it/s]\n","Epoch 11/30 loss 13.93783: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:34<00:00, 36.85it/s]\n","Epoch 11/30 val_loss 38.55943: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.09it/s]\n","Epoch 12/30 loss 13.28359: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:34<00:00, 36.98it/s]\n","Epoch 12/30 val_loss 38.96787: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 26.70it/s]\n","Epoch 13/30 loss 12.69608: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 36.65it/s]\n","Epoch 13/30 val_loss 39.32850: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.66it/s]\n","Epoch 14/30 loss 12.17222: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 36.56it/s]\n","Epoch 14/30 val_loss 39.54225: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.23it/s]\n","Epoch 15/30 loss 11.70690: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:34<00:00, 37.05it/s]\n","Epoch 15/30 val_loss 40.02328: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.79it/s]\n","Epoch 16/30 loss 11.27635: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.96it/s]\n","Epoch 16/30 val_loss 40.33026: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 26.11it/s]\n","Epoch 17/30 loss 10.88838: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 35.92it/s]\n","Epoch 17/30 val_loss 40.86835: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.56it/s]\n","Epoch 18/30 loss 10.54930: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:38<00:00, 33.32it/s]\n","Epoch 18/30 val_loss 40.82035: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 21.23it/s]\n","Epoch 19/30 loss 10.23883: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:38<00:00, 33.38it/s]\n","Epoch 19/30 val_loss 41.46555: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 22.81it/s]\n","Epoch 20/30 loss 9.94837: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:38<00:00, 33.23it/s]\n","Epoch 20/30 val_loss 41.87751: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.79it/s]\n","Epoch 21/30 loss 9.68863: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:38<00:00, 33.36it/s]\n","Epoch 21/30 val_loss 42.44731: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 23.10it/s]\n","Epoch 22/30 loss 9.43615: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:36<00:00, 35.62it/s]\n","Epoch 22/30 val_loss 42.24100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.85it/s]\n","Epoch 23/30 loss 9.22299: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 36.17it/s]\n","Epoch 23/30 val_loss 42.61219: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.87it/s]\n","Epoch 24/30 loss 9.02108: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 36.19it/s]\n","Epoch 24/30 val_loss 43.11875: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.16it/s]\n","Epoch 25/30 loss 8.81701: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:35<00:00, 36.28it/s]\n","Epoch 25/30 val_loss 43.55333: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.46it/s]\n","Epoch 26/30 loss 8.63820: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:36<00:00, 35.32it/s]\n","Epoch 26/30 val_loss 43.96737: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.69it/s]\n","Epoch 27/30 loss 8.48335: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:37<00:00, 34.14it/s]\n","Epoch 27/30 val_loss 43.99380: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 25.51it/s]\n","Epoch 28/30 loss 8.31917: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:36<00:00, 34.98it/s]\n","Epoch 28/30 val_loss 44.61350: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 24.16it/s]\n","Epoch 29/30 loss 8.17593: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:37<00:00, 33.82it/s]\n","Epoch 29/30 val_loss 44.54145: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 22.95it/s]\n","Epoch 30/30 loss 8.03488: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1285/1285 [00:44<00:00, 28.56it/s]\n","Epoch 30/30 val_loss 44.61531: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:03<00:00, 20.75it/s]\n"]}],"source":["fit(encoder, decoder, dataloader, epochs=30)"]},{"cell_type":"markdown","metadata":{"id":"AcI2LHpqOo9T"},"source":["## Generando traducciones"]},{"cell_type":"markdown","metadata":{"id":"xT552a3COo9T"},"source":["Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del ingl√©s al castellano de la siguiente manera. "]},{"cell_type":"code","execution_count":29,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T13:12:31.925887Z","start_time":"2020-09-04T13:12:31.915888Z"},"id":"kujnudbLOo9T","outputId":"3abd8da8-8acd-44d7-e422-fc37dbc0d480"},"outputs":[{"data":{"text/plain":["(['no', 'way', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'],\n"," ['', 'imposible', '!', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["input_sentence, output_sentence = dataset['train'][100]\n","input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"]},{"cell_type":"code","execution_count":30,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T13:12:32.615887Z","start_time":"2020-09-04T13:12:32.598887Z"},"code_folding":[],"id":"Fl-ctbrbOo9T"},"outputs":[],"source":["def predict(input_sentence):\n","    # obtenemos el √∫ltimo estado oculto del encoder\n","    encoder_outputs, hidden = encoder(input_sentence.unsqueeze(0))\n","    # calculamos las salidas del decoder de manera recurrente\n","    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n","    # iteramos hasta que el decoder nos de el token <eos>\n","    outputs = []\n","    decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n","    i = 0\n","    while True:\n","        output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n","        decoder_attentions[i] = attn_weights.data\n","        i += 1\n","        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n","        outputs.append(decoder_input.cpu().item())\n","        if decoder_input.item() == output_lang.word2index['EOS']:\n","            break\n","    return output_lang.sentenceFromIndex(outputs), decoder_attentions"]},{"cell_type":"code","execution_count":31,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T13:12:32.789887Z","start_time":"2020-09-04T13:12:32.775888Z"},"id":"QiUa7TDdOo9T","outputId":"c345715e-d5c7-4d6b-c1e6-864fd7b82c2d"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\israe\\AppData\\Local\\Temp\\ipykernel_19072\\1161045504.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n"]},{"data":{"text/plain":["['', 'no', 'no', '!', '!', 'EOS']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["output_words, attn = predict(input_sentence)\n","output_words"]},{"cell_type":"markdown","metadata":{"id":"vMd2U7soOo9U"},"source":["## Visualizaci√≥n de atenci√≥n"]},{"cell_type":"markdown","metadata":{"id":"ThkRUHiFOo9U"},"source":["Una de las ventajas que nos da la capa de atenci√≥n es que nos permite visualizar en qu√© partes de los inputs se fija el modelo para generar cada una de las palabras en el output, dando un grado de explicabilidad a nuestro modelo (una propiedad siempre deseada en nuestro modelos de `Machine Learning`)."]},{"cell_type":"code","execution_count":32,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T13:12:33.513889Z","start_time":"2020-09-04T13:12:33.505889Z"},"id":"t592WD_bOo9U"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","def showAttention(input_sentence, output_words, attentions):\n","    lim1, lim2 = input_sentence.index('EOS')+1, output_words.index('EOS')+1\n","    fig = plt.figure(dpi=100)\n","    ax = fig.add_subplot(111)\n","    cax = ax.matshow(attentions[:lim2, :lim1].numpy(), cmap='bone')\n","    fig.colorbar(cax)\n","    # Set up axes\n","    ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n","    ax.set_yticklabels([' '] + output_words)\n","    # Show label at every tick\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    plt.show()"]},{"cell_type":"code","execution_count":33,"metadata":{"ExecuteTime":{"end_time":"2020-09-04T13:12:34.273921Z","start_time":"2020-09-04T13:12:34.160888Z"},"id":"bvGfgc3lOo9U","outputId":"5caad247-b6c3-480e-98d1-d5215bf7f25a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\israe\\AppData\\Local\\Temp\\ipykernel_19072\\429842870.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n","C:\\Users\\israe\\AppData\\Local\\Temp\\ipykernel_19072\\429842870.py:12: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n","  ax.set_yticklabels([' '] + output_words)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["showAttention(input_lang.sentenceFromIndex(input_sentence.tolist()), output_words, attn)"]},{"cell_type":"markdown","metadata":{"id":"VfgIx8S0Oo9U"},"source":["## Resumen"]},{"cell_type":"markdown","metadata":{"id":"ULljgmxLOo9U"},"source":["En este post hemos visto como introducir mecanismos de atenci√≥n en nuestra arquitectura `encoder-decoder`, los cuales permiten a nuestra red neuronal focalizarse en partes concretas de los *inputs* a la hora de generar los *outputs*. Esta nueva capa no solo puede mejorar nuestros modelos sino que adem√°s tambi√©n es interpretable, d√°ndonos una idea del razonamiento detr√°s de las predicciones de nuestro modelo. Las redes neuronales con mejores prestaciones a d√≠a de hoy en tareas de `NLP`, los `transformers`, est√°n basados enteramente en este tipo de capas de atenci√≥n. "]}],"metadata":{"accelerator":"GPU","colab":{"name":"Copia de nlp_transfer.ipynb","provenance":[{"file_id":"https://github.com/juansensio/blog/blob/master/041_attention/attention.ipynb","timestamp":1623568595986}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"233.594px"},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}
